{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd09a5a8542cb27bafda8ad502146f64ea5748f0e4a737bf208b783aec3cbbf6631",
   "display_name": "Python 3.9.4 64-bit ('deeplearningtorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from PIL import Image\r\n",
    "import pytesseract\r\n",
    "from pdf2image import convert_from_path\r\n",
    "import os\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "# Path of the pdf\r\n",
    "PDF_file = \"./data/leopold-nih-foia-anthony-fauci-emails.pdf\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#making directories for intermediate statges\r\n",
    "os.mkdir('./data/pdfsplits/')\r\n",
    "os.mkdir('./data/jpgs/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#splitting pdf\r\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\r\n",
    "\r\n",
    "inputpdf = PdfFileReader(open(PDF_file, \"rb\"))\r\n",
    "\r\n",
    "for i in range(inputpdf.numPages):\r\n",
    "    output = PdfFileWriter()\r\n",
    "    output.addPage(inputpdf.getPage(i))\r\n",
    "    with open(\"./data/pdfsplits/document-page%s.pdf\" % (i+1), \"wb\") as outputStream:\r\n",
    "        output.write(outputStream)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import os\r\n",
    "dir_list = os.listdir('./data/pdfsplits/')\r\n",
    "len(dir_list)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3234"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#converting to jpgs\r\n",
    "image_counter = 1\r\n",
    "for i in range(len(dir_list)):\r\n",
    "    pages = convert_from_path('./data/pdfsplits/'+f'document-page{image_counter}.pdf', 500)\r\n",
    "    for page in pages:\r\n",
    "        filename = \"page_\"+str(image_counter)+\".jpg\"\r\n",
    "        page.save('./data/jpgs/'+filename, 'JPEG')\r\n",
    "    image_counter+=1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#couple cleaning functions that i found to process the image\r\n",
    "import tempfile\r\n",
    "\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "from PIL import Image\r\n",
    "\r\n",
    "IMAGE_SIZE = 1800\r\n",
    "BINARY_THREHOLD = 180\r\n",
    "\r\n",
    "def process_image_for_ocr(file_path):\r\n",
    "    temp_filename = set_image_dpi(file_path)\r\n",
    "    im_new = remove_noise_and_smooth(temp_filename)\r\n",
    "    return im_new\r\n",
    "\r\n",
    "def set_image_dpi(file_path):\r\n",
    "    im = Image.open(file_path)\r\n",
    "    length_x, width_y = im.size\r\n",
    "    factor = max(1, int(IMAGE_SIZE / length_x))\r\n",
    "    size = factor * length_x, factor * width_y\r\n",
    "    im_resized = im.resize(size, Image.ANTIALIAS)\r\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')\r\n",
    "    temp_filename = temp_file.name\r\n",
    "    im_resized.save(temp_filename, dpi=(600, 600))\r\n",
    "    return temp_filename\r\n",
    "\r\n",
    "def image_smoothening(img):\r\n",
    "    ret1, th1 = cv2.threshold(img, BINARY_THREHOLD, 255, cv2.THRESH_BINARY)\r\n",
    "    ret2, th2 = cv2.threshold(th1, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\r\n",
    "    blur = cv2.GaussianBlur(th2, (1, 1), 0)\r\n",
    "    ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\r\n",
    "    return th3\r\n",
    "def remove_noise_and_smooth(file_name):\r\n",
    "    img = cv2.imread(file_name, 0)\r\n",
    "    filtered = cv2.adaptiveThreshold(img.astype(np.uint8), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 41,\r\n",
    "                                     3)\r\n",
    "    kernel = np.ones((1, 1), np.uint8)\r\n",
    "    opening = cv2.morphologyEx(filtered, cv2.MORPH_OPEN, kernel)\r\n",
    "    closing = cv2.morphologyEx(opening, cv2.MORPH_CLOSE, kernel)\r\n",
    "    img = image_smoothening(img)\r\n",
    "    or_image = cv2.bitwise_or(img, closing)\r\n",
    "    return or_image\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#making the text files\r\n",
    "for i in range(len(dir_list)):\r\n",
    "    out_below = pytesseract.image_to_string(process_image_for_ocr(f'./data/jpgs/page_{i+1}.jpg'))\r\n",
    "    with open(f'./data/text/page_{i+1}.txt','w') as f:\r\n",
    "        f.write(out_below)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}